{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing IMDB Data in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "#import keras\n",
    "#import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data\n",
    "This dataset comes preloaded with Keras, so one simple command will get us training and testing data. There is a parameter for how many words we want to look at. We've set it at 1000, but feel free to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data (it's preloaded in Keras)\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=5000)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "word_to_int = imdb.get_word_index()\n",
    "int_to_word = {i:w for w,i in word_to_int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_word_review(review):\n",
    "    '''Takes a list of ints as a review and converts it to a worded review'''\n",
    "    worded_review = ''\n",
    "    for i in review:\n",
    "        worded_review += int_to_word[i] + ' '\n",
    "    return worded_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Word to Int Dictionary from Keras\n",
    "This reveals a problem with the review data set. When converting the ints to words, the reviews turn out to be garbage...  No wonder I was having trouble below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review # 15:\n",
      "the is rate hope br my out double this is again country known desperation memories is and and anyway standard planet poem late and to performance not his there's is carrey and just being film jerry in jerry baby and to murders this is and to have into one as fall issues but is on was had can't is dark some br of needed based just table and and of little with barely may and to country presentation don't plot and he 12 in could is opportunity lie wood are is late helps ii wanted not nothing from lost and i'm and jenny as with keep but and and that effects just is remember b was off use of and merely and this of ultimate old \n",
      "Review # 45:\n",
      "the this as aids 2000 as and more he clever and to more he and and expected are clever and of and to clever symbolism one is and opinion decision and and it is clever symbolism is and aids and film is method are and and and is and of energy br about ways and and and were strength most of and and to much and locations and is prince conspiracy for and of and and and br and to and not and there is nudity of and there is and really and and and handed in of night shallow would and this decision i i and there best kept movie from why an of film is worse i i they is slightly bad can is suppose everybody br of and recommend driven complaint about could she this is antics carrying and it is and and decision role in does and suppose go about property opens of clever and humor go of and book i'm of and br scarecrow wallace but and frame would role and film of clever symbolism no even buddy set this of complete its and scenes of clever symbolism cinematographer in own of and is act this circumstances about and i'm of map favorite br and and and tries jane in just about not would thing just jane winning not up five in of and for evil members no of clever symbolism case and of and of and comedies is and of and big and first is clever symbolism it's is occasional most of clever symbolism head of and law watch were perry and and local decision can actual really it of and br and were freeman film of and for about under eyes about movie director up been is and br and comedy think and ian and fantasy it is and would remote of arnold br flashbacks and exception and la decision in one by amazing and about and and and make property first of melting i i of because clever symbolism of because and and where under robin nyc and to and individuals worse sidney and styles and out just think clever symbolism to and one is and opinion movies go and la decision but did and la robin but recommend not series acting series of and br according she this really which is under into i'd are is very under what of forever in learn she of martial samurai provided become they is and of clever symbolism and of because clever symbolism watch and and this of and and pre fantasy it is and decision role in single of female most robin believe anything or never don it and to there so and and robin game in matter those who lots about sally movie and and of and 1950s or about wish and and avoid are and sadly tiresome not of and decision thank evil still been robin still been is clever symbolism mention some for about sally he is and br noticed i i and decent at process in having because i i seem this of amazing nice and and and meaningful for decision there is suppose movies to as and re clever and there of break in i'd and this of nearly thriller what that his example there and decent at process in having because this is for even worse after expecting \n"
     ]
    }
   ],
   "source": [
    "print('Review # 15:')\n",
    "print(int_word_review(x_total[15]))\n",
    "print('Review # 45:')\n",
    "print(int_word_review(x_total[45]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A. Combine test and training data so we can filter out noise\n",
    "Use log polarization metric from Trask's Sentiment NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4998\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "positive_counts = Counter()\n",
    "negative_counts = Counter()\n",
    "total_counts = Counter()\n",
    "\n",
    "x_total = np.concatenate((x_train,x_test))\n",
    "y_total = np.concatenate((y_train,y_test))\n",
    "for idx, review in np.ndenumerate(x_total):\n",
    "    for word in review:\n",
    "        if y_total[idx[0]] == 0:\n",
    "            positive_counts[word] +=1\n",
    "            total_counts[word] +=1\n",
    "        elif y_total[idx[0]] == 1:\n",
    "            negative_counts[word] +=1\n",
    "            total_counts[word] +=1\n",
    "\n",
    "# Build Ratios\n",
    "min_count = 50\n",
    "pos_neg_ratios = Counter()\n",
    "\n",
    "for word, count in list(total_counts.most_common()):\n",
    "    if count > min_count:\n",
    "        ratio = positive_counts[word] / float(negative_counts[word]+1)\n",
    "        if ratio > 1:\n",
    "            pos_neg_ratios[word] = np.log(ratio)\n",
    "        else:\n",
    "            pos_neg_ratios[word] = -np.log((1 / (ratio + 0.01)))\n",
    "\n",
    "print(len(pos_neg_ratios))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. B. Distribution of Ratios\n",
    "Now, examine the distribution of ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkjpn0EtS5wx6SeqcQS9JnTPoJalz/w8nvQ1nAx6N1QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x275102bb6a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(list(pos_neg_ratios.values()), bins=75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cutoff the words with an absolute value of <= 1 STD (STD = 0.47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity Cutoff =  0.00575069917747\n",
      "Sample Review before:  [1, 5, 14, 9, 6, 55, 1193, 22, 13, 203, 30, 355, 21, 14, 9, 4, 236, 22, 121, 13, 1192, 2967, 3622, 35, 779, 284, 37, 2, 4, 217, 5, 2132, 6, 749, 10, 10, 2636, 4252, 5, 2931, 4517, 26, 82, 321, 36, 26, 2, 5, 4960, 2, 1786, 8, 358, 4, 704, 117, 122, 36, 124, 51, 62, 593, 375, 10, 10, 4, 1381, 5, 732, 26, 821, 5, 1249, 14, 16, 159, 4, 504, 7, 3728, 4913, 10, 10, 51, 9, 91, 1193, 44, 14, 22, 9, 4, 192, 15, 1370, 40, 14, 131, 1778, 11, 938, 704, 3834, 131, 2, 543, 84, 12, 9, 220, 6, 1117, 5, 6, 320, 237, 4, 3286, 325, 10, 10, 25, 80, 358, 14, 22, 12, 16, 814, 11, 4, 3968, 2, 7, 1226, 2, 63, 131, 1778, 43, 92, 1278, 501, 15, 8, 2, 2, 15, 1609, 131, 47, 24, 77, 2, 237, 2, 2, 158, 158]\n",
      "Sample Review before:  the to as it is time failed you was action at enjoy not as it of performance you know was sick obsession butt so york shows like and of almost to kung is above i i metal surfing to obnoxious ballet he other shot from he and to definition and torture in use of class over off from does when story ago couple i i of impression to oscar he doubt to honestly as with new of they're br grandmother awfully i i when it its failed has as you it of enough for details just as these suspect this keeps class commercials these and myself great that it family is concept to is star he's of stunts black i i have into use as you that with means this of clothing and br touch and really these suspect out then rare days for in and and for angry these there his will and he's and and didn't didn't \n",
      "Old maximum review length = 2494\n",
      "New maximum review length = 2467\n",
      "Sample Review after:  [1, 5, 14, 9, 6, 55, 1193, 22, 13, 203, 30, 355, 21, 14, 9, 4, 236, 22, 121, 13, 1192, 2967, 3622, 35, 779, 284, 37, 2, 4, 217, 5, 2132, 6, 749, 10, 10, 2636, 4252, 5, 2931, 4517, 26, 82, 321, 36, 26, 2, 5, 4960, 2, 1786, 8, 358, 4, 704, 117, 122, 36, 124, 51, 62, 593, 375, 10, 10, 4, 1381, 5, 732, 26, 821, 5, 1249, 14, 16, 4, 504, 7, 3728, 4913, 10, 10, 51, 9, 91, 1193, 44, 14, 22, 9, 4, 192, 15, 1370, 40, 14, 131, 1778, 11, 938, 704, 3834, 131, 2, 543, 84, 12, 9, 220, 6, 1117, 5, 6, 320, 237, 4, 3286, 325, 10, 10, 25, 80, 358, 14, 22, 12, 16, 814, 11, 4, 3968, 2, 7, 1226, 2, 63, 131, 1778, 43, 92, 1278, 501, 15, 8, 2, 2, 15, 1609, 131, 47, 24, 77, 2, 237, 2, 2, 158, 158]\n",
      "Sample Review after:  the to as it is time failed you was action at enjoy not as it of performance you know was sick obsession butt so york shows like and of almost to kung is above i i metal surfing to obnoxious ballet he other shot from he and to definition and torture in use of class over off from does when story ago couple i i of impression to oscar he doubt to honestly as with of they're br grandmother awfully i i when it its failed has as you it of enough for details just as these suspect this keeps class commercials these and myself great that it family is concept to is star he's of stunts black i i have into use as you that with means this of clothing and br touch and really these suspect out then rare days for in and and for angry these there his will and he's and and didn't didn't \n"
     ]
    }
   ],
   "source": [
    "polarity_cutoff = np.std(list(pos_neg_ratios.values())) * 0.01\n",
    "print('Polarity Cutoff = ', polarity_cutoff)\n",
    "max_review = 0\n",
    "max_new = 0\n",
    "print('Sample Review before: ',x_total[150])\n",
    "print('Sample Review before: ',int_word_review(x_total[150]))\n",
    "for idx, review in np.ndenumerate(x_total):\n",
    "    max_review = max(max_review,len(review))\n",
    "    new_review = []\n",
    "    for word in review:\n",
    "        if np.abs(pos_neg_ratios[word]) > polarity_cutoff:\n",
    "            new_review.append(word)\n",
    "    x_total[idx] = new_review\n",
    "    max_new = max(max_new,len(new_review))\n",
    "print('Old maximum review length =', max_review)\n",
    "print('New maximum review length =', max_new)\n",
    "print('Sample Review after: ',x_total[150])\n",
    "print('Sample Review after: ',int_word_review(x_total[150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length= 2467\n"
     ]
    }
   ],
   "source": [
    "xmax = 0\n",
    "for x in x_total:\n",
    "    xmax = max(xmax,len(x))\n",
    "print('Maximum length=',xmax)\n",
    "num_words = xmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split it up again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_total, y_total, test_size = 0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examining the data\n",
    "Notice that the data has been already pre-processed, where all the words have numbers, and the reviews come in as a vector with the words that the review contains. For example, if the word 'the' is the first one in our dictionary, and a review contains the word 'the', then there is a 1 in the corresponding vector.\n",
    "\n",
    "The output comes as a vector of 1's and 0's, where 1 is a positive sentiment for the review, and 0 is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 308, 2, 16, 321, 17, 488, 11, 4, 20, 34, 4, 172, 403, 13, 92, 6, 128, 1146, 100, 28, 77, 224, 4, 20, 16, 163, 246, 1301, 488, 9, 6, 55, 406, 2344, 48, 68, 9, 2, 2, 9, 897, 11, 41, 217, 17, 6, 2422, 17, 59, 271, 39, 2793, 8, 2607, 988, 1487, 5, 670, 2, 26, 87, 17, 1486, 2, 257, 2, 23, 2, 4, 2, 7, 4, 2344, 23, 703, 257, 7, 4, 696, 177, 9, 389, 262, 4, 922, 255, 13, 81, 24, 124, 41, 268, 403, 37, 299, 2, 452, 31, 7, 4, 2, 102, 13, 28, 110, 11, 153, 13, 100, 106, 14, 20, 4193, 7, 211]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. One-hot encoding the output\n",
    "Here, we'll turn the input vectors into (0,1)-vectors. For example, if the pre-processed vector contains the number 14, then in the processed vector, the 14th entry will be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 308,\n",
       " 2,\n",
       " 16,\n",
       " 321,\n",
       " 17,\n",
       " 488,\n",
       " 11,\n",
       " 4,\n",
       " 20,\n",
       " 34,\n",
       " 4,\n",
       " 172,\n",
       " 403,\n",
       " 13,\n",
       " 92,\n",
       " 6,\n",
       " 128,\n",
       " 1146,\n",
       " 100,\n",
       " 28,\n",
       " 77,\n",
       " 224,\n",
       " 4,\n",
       " 20,\n",
       " 16,\n",
       " 163,\n",
       " 246,\n",
       " 1301,\n",
       " 488,\n",
       " 9,\n",
       " 6,\n",
       " 55,\n",
       " 406,\n",
       " 2344,\n",
       " 48,\n",
       " 68,\n",
       " 9,\n",
       " 2,\n",
       " 2,\n",
       " 9,\n",
       " 897,\n",
       " 11,\n",
       " 41,\n",
       " 217,\n",
       " 17,\n",
       " 6,\n",
       " 2422,\n",
       " 17,\n",
       " 59,\n",
       " 271,\n",
       " 39,\n",
       " 2793,\n",
       " 8,\n",
       " 2607,\n",
       " 988,\n",
       " 1487,\n",
       " 5,\n",
       " 670,\n",
       " 2,\n",
       " 26,\n",
       " 87,\n",
       " 17,\n",
       " 1486,\n",
       " 2,\n",
       " 257,\n",
       " 2,\n",
       " 23,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 7,\n",
       " 4,\n",
       " 2344,\n",
       " 23,\n",
       " 703,\n",
       " 257,\n",
       " 7,\n",
       " 4,\n",
       " 696,\n",
       " 177,\n",
       " 9,\n",
       " 389,\n",
       " 262,\n",
       " 4,\n",
       " 922,\n",
       " 255,\n",
       " 13,\n",
       " 81,\n",
       " 24,\n",
       " 124,\n",
       " 41,\n",
       " 268,\n",
       " 403,\n",
       " 37,\n",
       " 299,\n",
       " 2,\n",
       " 452,\n",
       " 31,\n",
       " 7,\n",
       " 4,\n",
       " 2,\n",
       " 102,\n",
       " 13,\n",
       " 28,\n",
       " 110,\n",
       " 11,\n",
       " 153,\n",
       " 13,\n",
       " 100,\n",
       " 106,\n",
       " 14,\n",
       " 20,\n",
       " 4193,\n",
       " 7,\n",
       " 211]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  1. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding the output into vector mode, each of length 1000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll also one-hot encode the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2467) (25000, 2)\n",
      "(25000, 2467) (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding the output\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the  model architecture\n",
    "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Number of Words: 2467\n",
      "Training Data shape: (25000, 2467)\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "print('Max Number of Words:', num_words)\n",
    "print('Training Data shape:',x_train.shape)\n",
    "print('Number of classes:',num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 1233)              3043044   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1233)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 2468      \n",
      "=================================================================\n",
      "Total params: 3,045,512\n",
      "Trainable params: 3,045,512\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TODO: Build the model architecture\n",
    "# Building the model\n",
    "model = Sequential()\n",
    "model.add(Dense(max(num_words//2,100), activation='relu', input_dim=num_words))\n",
    "model.add(Dropout(.3))\n",
    "#model.add(Dense(80, activation='relu'))\n",
    "#model.add(Dropout(.2))\n",
    "#model.add(Dense(num_words//2, activation='relu'))\n",
    "#model.add(Dropout(.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# TODO: Compile the model using a loss function and an optimizer.\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the model\n",
    "Run the model here. Experiment with different batch_size, and number of epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 4s - loss: 0.3608 - acc: 0.8502     \n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 3s - loss: 0.2586 - acc: 0.9013     \n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 3s - loss: 0.2207 - acc: 0.9255     \n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 3s - loss: 0.1755 - acc: 0.9456     \n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 3s - loss: 0.1402 - acc: 0.9628     \n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 3s - loss: 0.1101 - acc: 0.9727     \n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 3s - loss: 0.0902 - acc: 0.9806     \n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 3s - loss: 0.0710 - acc: 0.9855     \n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 3s - loss: 0.0596 - acc: 0.9884     \n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 3s - loss: 0.0493 - acc: 0.9908     \n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 3s - loss: 0.0455 - acc: 0.9928     \n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 3s - loss: 0.0420 - acc: 0.9939     \n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 3s - loss: 0.0339 - acc: 0.9954     \n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 3s - loss: 0.0299 - acc: 0.9961     \n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 3s - loss: 0.0328 - acc: 0.9957     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2750b131780>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Run the model. Feel free to experiment with different batch sizes and number of epochs.\n",
    "\n",
    "model.fit(x_train, y_train, epochs=15, batch_size=48, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the model\n",
    "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.99848\n",
      "Test Accuracy:  0.86868\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Train Accuracy: \", score[1])\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2467"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 100)               246800    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 247,002\n",
      "Trainable params: 247,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "5s - loss: 0.3451 - acc: 0.8545 - val_loss: 0.2965 - val_acc: 0.8786\n",
      "Epoch 2/10\n",
      "5s - loss: 0.2696 - acc: 0.8936 - val_loss: 0.3073 - val_acc: 0.8816\n",
      "Epoch 3/10\n",
      "4s - loss: 0.2527 - acc: 0.9039 - val_loss: 0.3196 - val_acc: 0.8806\n",
      "Epoch 4/10\n",
      "5s - loss: 0.2405 - acc: 0.9128 - val_loss: 0.3287 - val_acc: 0.8798\n",
      "Epoch 5/10\n",
      "5s - loss: 0.2296 - acc: 0.9177 - val_loss: 0.3437 - val_acc: 0.8796\n",
      "Epoch 6/10\n",
      "5s - loss: 0.2170 - acc: 0.9238 - val_loss: 0.3557 - val_acc: 0.8802\n",
      "Epoch 7/10\n",
      "5s - loss: 0.2067 - acc: 0.9282 - val_loss: 0.3752 - val_acc: 0.8775\n",
      "Epoch 8/10\n",
      "5s - loss: 0.2010 - acc: 0.9331 - val_loss: 0.4172 - val_acc: 0.8750\n",
      "Epoch 9/10\n",
      "4s - loss: 0.1896 - acc: 0.9364 - val_loss: 0.4222 - val_acc: 0.8772\n",
      "Epoch 10/10\n",
      "5s - loss: 0.1822 - acc: 0.9402 - val_loss: 0.4372 - val_acc: 0.8747\n"
     ]
    }
   ],
   "source": [
    "# Building the model architecture with one layer of length 100\n",
    "smodel = Sequential()\n",
    "#smodel.add(Dense(512, activation='relu', input_dim=num_words))\n",
    "smodel.add(Dense(100, activation='relu', input_dim=num_words))\n",
    "smodel.add(Dropout(0.3))\n",
    "smodel.add(Dense(num_classes, activation='softmax'))\n",
    "smodel.summary()\n",
    "\n",
    "# Compiling the model using categorical_crossentropy loss, and rmsprop optimizer.\n",
    "smodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Running and evaluating the model\n",
    "hist = smodel.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test), \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.955\n",
      "Test Accuracy:  0.87468\n"
     ]
    }
   ],
   "source": [
    "sscore = smodel.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Train Accuracy: \", sscore[1])\n",
    "sscore = smodel.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test Accuracy: \", sscore[1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing IMDB Data in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data\n",
    "This dataset comes preloaded with Keras, so one simple command will get us training and testing data. There is a parameter for how many words we want to look at. We've set it at 1000, but feel free to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "# Loading the data (it's preloaded in Keras)\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=1000)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. A. Combine test and training data so we can filter out noise\n",
    "Use log polarization metric from Trask's Sentiment NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "positive_counts = Counter()\n",
    "negative_counts = Counter()\n",
    "total_counts = Counter()\n",
    "\n",
    "x_total = np.concatenate((x_train,x_test))\n",
    "y_total = np.concatenate((y_train,y_test))\n",
    "for idx, review in np.ndenumerate(x_total):\n",
    "    for word in review:\n",
    "        if y_total[idx[0]] == 0:\n",
    "            positive_counts[word] +=1\n",
    "            total_counts[word] +=1\n",
    "        elif y_total[idx[0]] == 1:\n",
    "            negative_counts[word] +=1\n",
    "            total_counts[word] +=1\n",
    "\n",
    "# Build Ratios\n",
    "min_count = 50\n",
    "pos_neg_ratios = Counter()\n",
    "\n",
    "for word, count in list(total_counts.most_common()):\n",
    "    if count > min_count:\n",
    "        ratio = positive_counts[word] / float(negative_counts[word]+1)\n",
    "        if ratio > 1:\n",
    "            pos_neg_ratios[word] = np.log(ratio)\n",
    "        else:\n",
    "            pos_neg_ratios[word] = -np.log((1 / (ratio + 0.01)))\n",
    "\n",
    "print(len(pos_neg_ratios))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. B. Distribution of Ratios\n",
    "Now, examine the distribution of ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cc9dfbc128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(list(pos_neg_ratios.values()), bins=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cutoff the words with an absolute value of <= 1 STD (STD = 0.47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity Cutoff =  0.352738679334\n",
      "Sample Review before:  [1, 5, 14, 9, 6, 55, 2, 22, 13, 203, 30, 355, 21, 14, 9, 4, 236, 22, 121, 13, 2, 2, 2, 35, 779, 284, 37, 2, 4, 217, 5, 2, 6, 749, 10, 10, 2, 2, 5, 2, 2, 26, 82, 321, 36, 26, 2, 5, 2, 2, 2, 8, 358, 4, 704, 117, 122, 36, 124, 51, 62, 593, 375, 10, 10, 4, 2, 5, 732, 26, 821, 5, 2, 14, 16, 159, 4, 504, 7, 2, 2, 10, 10, 51, 9, 91, 2, 44, 14, 22, 9, 4, 192, 15, 2, 40, 14, 131, 2, 11, 938, 704, 2, 131, 2, 543, 84, 12, 9, 220, 6, 2, 5, 6, 320, 237, 4, 2, 325, 10, 10, 25, 80, 358, 14, 22, 12, 16, 814, 11, 4, 2, 2, 7, 2, 2, 63, 131, 2, 43, 92, 2, 501, 15, 8, 2, 2, 15, 2, 131, 47, 24, 77, 2, 237, 2, 2, 158, 158]\n",
      "Old maximum review length = 2494\n",
      "New maximum review length = 197\n",
      "Sample Review after:  [355, 217, 82, 321, 358, 704, 821, 131, 704, 131, 543, 320, 325, 358, 131, 43, 92, 131]\n"
     ]
    }
   ],
   "source": [
    "polarity_cutoff = np.std(list(pos_neg_ratios.values())) * .75\n",
    "print('Polarity Cutoff = ', polarity_cutoff)\n",
    "max_review = 0\n",
    "max_new = 0\n",
    "print('Sample Review before: ',x_total[150])\n",
    "for idx, review in np.ndenumerate(x_total):\n",
    "    max_review = max(max_review,len(review))\n",
    "    new_review = []\n",
    "    for word in review:\n",
    "        if np.abs(pos_neg_ratios[word]) > polarity_cutoff:\n",
    "            new_review.append(word)\n",
    "    x_total[idx] = new_review\n",
    "    max_new = max(max_new,len(new_review))\n",
    "print('Old maximum review length =', max_review)\n",
    "print('New maximum review length =', max_new)\n",
    "print('Sample Review after: ',x_total[150])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length= 197\n"
     ]
    }
   ],
   "source": [
    "xmax = 0\n",
    "for x in x_total:\n",
    "    xmax = max(xmax,len(x))\n",
    "print('Maximum length=',xmax)\n",
    "num_words = xmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split it up again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_total, y_total, test_size = 0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examining the data\n",
    "Notice that the data has been already pre-processed, where all the words have numbers, and the reviews come in as a vector with the words that the review contains. For example, if the word 'the' is the first one in our dictionary, and a review contains the word 'the', then there is a 1 in the corresponding vector.\n",
    "\n",
    "The output comes as a vector of 1's and 0's, where 1 is a positive sentiment for the review, and 0 is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[308, 321, 92, 128, 100, 406, 897, 217, 988, 670, 87, 257, 257, 696, 389, 262, 922, 299, 452, 153, 100]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. One-hot encoding the output\n",
    "Here, we'll turn the input vectors into (0,1)-vectors. For example, if the pre-processed vector contains the number 14, then in the processed vector, the 14th entry will be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[308,\n",
       " 321,\n",
       " 92,\n",
       " 128,\n",
       " 100,\n",
       " 406,\n",
       " 897,\n",
       " 217,\n",
       " 988,\n",
       " 670,\n",
       " 87,\n",
       " 257,\n",
       " 257,\n",
       " 696,\n",
       " 389,\n",
       " 262,\n",
       " 922,\n",
       " 299,\n",
       " 452,\n",
       " 153,\n",
       " 100]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding the output into vector mode, each of length 1000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n",
    "x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll also one-hot encode the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 197) (25000, 2)\n",
      "(25000, 197) (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encoding the output\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the  model architecture\n",
    "Build a model here using sequential. Feel free to experiment with different layers and sizes! Also, experiment adding dropout to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 197)               39006     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 197)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 792       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 39,808\n",
      "Trainable params: 39,808\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# TODO: Build the model architecture\n",
    "# Building the model\n",
    "model = Sequential()\n",
    "model.add(Dense(num_words, activation='relu', input_dim=num_words))\n",
    "model.add(Dropout(.3))\n",
    "#model.add(Dense(10, activation='relu'))\n",
    "#model.add(Dropout(.2))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# TODO: Compile the model using a loss function and an optimizer.\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the model\n",
    "Run the model here. Experiment with different batch_size, and number of epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "25000/25000 [==============================] - 1s - loss: 0.5710 - acc: 0.7042     \n",
      "Epoch 2/16\n",
      "25000/25000 [==============================] - 0s - loss: 0.5533 - acc: 0.7194     \n",
      "Epoch 3/16\n",
      "25000/25000 [==============================] - 0s - loss: 0.5486 - acc: 0.7214     \n",
      "Epoch 4/16\n",
      "25000/25000 [==============================] - 1s - loss: 0.5454 - acc: 0.7209     \n",
      "Epoch 5/16\n",
      "25000/25000 [==============================] - 0s - loss: 0.5440 - acc: 0.7229     \n",
      "Epoch 6/16\n",
      "25000/25000 [==============================] - 0s - loss: 0.5409 - acc: 0.7276     \n",
      "Epoch 7/16\n",
      "25000/25000 [==============================] - 1s - loss: 0.5387 - acc: 0.7281     \n",
      "Epoch 8/16\n",
      "25000/25000 [==============================] - 1s - loss: 0.5379 - acc: 0.7278     \n",
      "Epoch 9/16\n",
      "25000/25000 [==============================] - 0s - loss: 0.5371 - acc: 0.7279     \n",
      "Epoch 10/16\n",
      "25000/25000 [==============================] - 0s - loss: 0.5355 - acc: 0.7314     \n",
      "Epoch 11/16\n",
      "25000/25000 [==============================] - 0s - loss: 0.5334 - acc: 0.7334     \n",
      "Epoch 12/16\n",
      "25000/25000 [==============================] - 0s - loss: 0.5327 - acc: 0.7339     \n",
      "Epoch 13/16\n",
      "25000/25000 [==============================] - 0s - loss: 0.5300 - acc: 0.7340     \n",
      "Epoch 14/16\n",
      "25000/25000 [==============================] - 0s - loss: 0.5286 - acc: 0.7393     \n",
      "Epoch 15/16\n",
      "25000/25000 [==============================] - 0s - loss: 0.5285 - acc: 0.7360     \n",
      "Epoch 16/16\n",
      "25000/25000 [==============================] - 0s - loss: 0.5273 - acc: 0.7359     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cc82972ba8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Run the model. Feel free to experiment with different batch sizes and number of epochs.\n",
    "\n",
    "model.fit(x_train, y_train, epochs=16, batch_size= 32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluating the model\n",
    "This will give you the accuracy of the model, as evaluated on the testing set. Can you get something over 85%?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.74816\n",
      "Test Accuracy:  0.71424\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Train Accuracy: \", score[1])\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 512)               101376    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 102,402\n",
      "Trainable params: 102,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "1s - loss: 0.5605 - acc: 0.7118 - val_loss: 0.5556 - val_acc: 0.7165\n",
      "Epoch 2/10\n",
      "1s - loss: 0.5506 - acc: 0.7208 - val_loss: 0.5507 - val_acc: 0.7194\n",
      "Epoch 3/10\n",
      "1s - loss: 0.5482 - acc: 0.7245 - val_loss: 0.5511 - val_acc: 0.7183\n",
      "Epoch 4/10\n",
      "1s - loss: 0.5458 - acc: 0.7207 - val_loss: 0.5513 - val_acc: 0.7187\n",
      "Epoch 5/10\n",
      "1s - loss: 0.5448 - acc: 0.7261 - val_loss: 0.5582 - val_acc: 0.7140\n",
      "Epoch 6/10\n",
      "1s - loss: 0.5433 - acc: 0.7246 - val_loss: 0.5522 - val_acc: 0.7187\n",
      "Epoch 7/10\n",
      "1s - loss: 0.5427 - acc: 0.7280 - val_loss: 0.5543 - val_acc: 0.7172\n",
      "Epoch 8/10\n",
      "1s - loss: 0.5412 - acc: 0.7267 - val_loss: 0.5557 - val_acc: 0.7156\n",
      "Epoch 9/10\n",
      "1s - loss: 0.5408 - acc: 0.7262 - val_loss: 0.5551 - val_acc: 0.7166\n",
      "Epoch 10/10\n",
      "1s - loss: 0.5397 - acc: 0.7273 - val_loss: 0.5588 - val_acc: 0.7172\n"
     ]
    }
   ],
   "source": [
    "# Building the model architecture with one layer of length 100\n",
    "smodel = Sequential()\n",
    "smodel.add(Dense(512, activation='relu', input_dim=num_words))\n",
    "smodel.add(Dropout(0.3))\n",
    "smodel.add(Dense(num_classes, activation='softmax'))\n",
    "smodel.summary()\n",
    "\n",
    "# Compiling the model using categorical_crossentropy loss, and rmsprop optimizer.\n",
    "smodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Running and evaluating the model\n",
    "hist = smodel.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test), \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:  0.73416\n",
      "Test Accuracy:  0.71724\n"
     ]
    }
   ],
   "source": [
    "sscore = smodel.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Train Accuracy: \", sscore[1])\n",
    "sscore = smodel.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test Accuracy: \", sscore[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
